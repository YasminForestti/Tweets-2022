{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografia:\n",
    "\n",
    "Para o modelo de submissão:\n",
    "https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial/notebook\n",
    "\n",
    "Conceitos de ML:\n",
    "https://tatianaesc.medium.com/machine-learning-conceitos-e-modelos-f0373bf4f445\n",
    "\n",
    "Visualização:\n",
    "https://realpython.com/twitter-sentiment-python-docker-elasticsearch-kibana/\n",
    "\n",
    "Conceito VADER:\n",
    "https://ojs.aaai.org/index.php/ICWSM/article/view/14550/14399"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Limpeza das palavras: tirar as stop words, lematização, Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Vizualização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yfrom\\Desktop\\Analytica\\Tweets-2022\\ProcessamentodeLinguagemNatural.ipynb Célula: 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yfrom/Desktop/Analytica/Tweets-2022/ProcessamentodeLinguagemNatural.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yfrom/Desktop/Analytica/Tweets-2022/ProcessamentodeLinguagemNatural.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yfrom/Desktop/Analytica/Tweets-2022/ProcessamentodeLinguagemNatural.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yfrom/Desktop/Analytica/Tweets-2022/ProcessamentodeLinguagemNatural.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_extraction, linear_model, model_selection, preprocessing\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yfrom/Desktop/Analytica/Tweets-2022/ProcessamentodeLinguagemNatural.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msentiment\u001b[39;00m \u001b[39mimport\u001b[39;00m SentimentIntensityAnalyzer\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\__init__.py:128\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m     subprocess\u001b[39m.\u001b[39mPopen \u001b[39m=\u001b[39m _fake_Popen\n\u001b[0;32m    122\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[39m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollocations\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m decorator, memoize\n\u001b[0;32m    130\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatstruct\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\collocations.py:39\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m ngrams\n\u001b[0;32m     38\u001b[0m \u001b[39m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     ContingencyMeasures,\n\u001b[0;32m     41\u001b[0m     BigramAssocMeasures,\n\u001b[0;32m     42\u001b[0m     TrigramAssocMeasures,\n\u001b[0;32m     43\u001b[0m     QuadgramAssocMeasures,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspearman\u001b[39;00m \u001b[39mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[0;32m     48\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAbstractCollocationFinder\u001b[39;00m(\u001b[39mobject\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\metrics\\__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Natural Language Toolkit: Metrics\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Copyright (C) 2001-2020 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mNLTK Metrics\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[39mClasses and methods for scoring processing modules.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscores\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     accuracy,\n\u001b[0;32m     18\u001b[0m     precision,\n\u001b[0;32m     19\u001b[0m     recall,\n\u001b[0;32m     20\u001b[0m     f_measure,\n\u001b[0;32m     21\u001b[0m     log_likelihood,\n\u001b[0;32m     22\u001b[0m     approxrand,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfusionmatrix\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfusionMatrix\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     edit_distance,\n\u001b[0;32m     27\u001b[0m     edit_distance_align,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     fractional_presence,\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\metrics\\scores.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m reduce\n\u001b[0;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m betai\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     betai \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\stats\\__init__.py:467\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m \n\u001b[0;32m    463\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    466\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 467\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    468\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    469\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\stats\\distributions.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m (comb, chndtr, entr, xlogy, ive)\n\u001b[0;32m     22\u001b[0m \u001b[39m# for root finding for continuous distribution ppf, and max likelihood\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# estimation\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize\n\u001b[0;32m     26\u001b[0m \u001b[39m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m integrate\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:211\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(name):\n\u001b[0;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m submodules:\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m _importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mscipy.\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    212\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\__init__.py:401\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m=====================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mOptimization and root finding (:mod:`scipy.optimize`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m \n\u001b[0;32m    398\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 401\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_minimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    402\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_root\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    403\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_root_scalar\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_trustregion_krylov\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trust_krylov\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_trustregion_exact\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trustregion_exact\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_trustregion_constr\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m     28\u001b[0m \u001b[39m# constrained minimization\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lbfgsb_py\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_lbfgsb\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"This module contains the equality constrained SQP solver.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mminimize_trustregion_constr\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m      6\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m_minimize_trustregion_constr\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\minimize_trustregion_constr.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimizeResult\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_differentiable_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m ScalarFunction\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mequality_constrained_sqp\u001b[39;00m \u001b[39mimport\u001b[39;00m equality_constrained_sqp\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcanonical_constraint\u001b[39;00m \u001b[39mimport\u001b[39;00m (CanonicalConstraint,\n\u001b[0;32m     12\u001b[0m                                    initial_constraints_as_canonical)\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtr_interior_point\u001b[39;00m \u001b[39mimport\u001b[39;00m tr_interior_point\n",
      "File \u001b[1;32mc:\\Users\\yfrom\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\equality_constrained_sqp.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"Byrd-Omojokun Trust-Region SQP method.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m eye \u001b[39mas\u001b[39;00m speye\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mprojections\u001b[39;00m \u001b[39mimport\u001b[39;00m projections\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mqp_subproblem\u001b[39;00m \u001b[39mimport\u001b[39;00m modified_dogleg, projected_cg, box_intersections\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:930\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:387\u001b[0m, in \u001b[0;36mcache_from_source\u001b[1;34m(path, debug_override, optimization)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:129\u001b[0m, in \u001b[0;36m_path_split\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:129\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Minimização dos ruídos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets_train=train_df['text']\n",
    "sample_tweets_test=test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall\",\n",
    "\"he'll've\": \"he shall have\",\n",
    "\"he's\": \"he has\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall\",\n",
    "\"i'll've\": \"I shall have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"it's\": \"it is\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall\",\n",
    "\"it'll've\": \"it shall have\",\n",
    "\"it's\": \"it has\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall\",\n",
    "\"she'll've\": \"she shall have\",\n",
    "\"she's\": \"she has\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall\",\n",
    "\"they'll've\": \"they shall have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall\",\n",
    "\"what'll've\": \"what shall have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall\",\n",
    "\"who'll've\": \"who shall have\",\n",
    "\"who's\": \"who has\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall\",\n",
    "\"you'll've\": \"you shall have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contracao(sample_tweets):\n",
    "    sample = []\n",
    "    for tweets in sample_tweets:\n",
    "        for word in tweets.split():\n",
    "            if word.lower() in contractions:\n",
    "                tweets = tweets.replace(word, contractions[word.lower()])\n",
    "        sample.append(tweets)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets_train = pd.DataFrame(contracao(sample_tweets_train))\n",
    "sample_tweets_test = pd.DataFrame(contracao(sample_tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  our deeds are the reason of this earthquake ma...\n",
       "1              forest fire near la ronge sask canada\n",
       "2  all residents asked to shelter in place are be...\n",
       "3   people receive wildfires evacuation orders in...\n",
       "4  just got sent this photo from ruby alaska as s..."
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tweets_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpeza(tweets):\n",
    "    tweets = tweets.str.replace(r'@[a-zA-Z0-9_!.]{0,25}','')\n",
    "    tweets = tweets.str.replace('(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+','')\n",
    "    tweets = tweets.str.replace(r'[0-9]','')\n",
    "    tweets = tweets.replace(\"\\\\x89Û_\",'')\n",
    "    tweets = tweets.replace('\\x89ÛÓ','')\n",
    "    tweets = tweets.replace('\\x89ÛÓ','')\n",
    "    tweets = tweets.replace('\\x89ÛÒ','')\n",
    "    tweets = tweets.replace('\\x89Û','')\n",
    "    tweets = tweets.replace('\\x89Û÷','')\n",
    "    tweets = tweets.replace('\\x89ûï','')\n",
    "    tweets = tweets.replace('u','you')\n",
    "    #retirada de símbolos não alfanuméticos \n",
    "    tweets = tweets.str.replace(r\"['#=<>,.;:_?!(-)\\[\\]]\",\"\")\n",
    "    tweets = tweets.str.lower()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yfrom\\AppData\\Local\\Temp\\ipykernel_11196\\2165951979.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets = tweets.str.replace(r'@[a-zA-Z0-9_!.]{0,25}','')\n",
      "C:\\Users\\yfrom\\AppData\\Local\\Temp\\ipykernel_11196\\2165951979.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets = tweets.str.replace('(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+','')\n",
      "C:\\Users\\yfrom\\AppData\\Local\\Temp\\ipykernel_11196\\2165951979.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets = tweets.str.replace(r'[0-9]','')\n",
      "C:\\Users\\yfrom\\AppData\\Local\\Temp\\ipykernel_11196\\2165951979.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets = tweets.str.replace(r\"['#=<>,.;:_?!(-)\\[\\]]\",\"\")\n"
     ]
    }
   ],
   "source": [
    "sample_tweets_train = limpeza(sample_tweets_train[0])\n",
    "sample_tweets_test = limpeza(sample_tweets_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizar(tweets):\n",
    "    sample_tweets= []\n",
    "    for tweet in tweets:\n",
    "        filtered_list = []\n",
    "        words_lemmatized = []\n",
    "        words = []\n",
    "        words = word_tokenize(tweet) \n",
    "        words_lemmatized = [lemmatizer.lemmatize(word) for word in words] \n",
    "        for word in words_lemmatized:\n",
    "            if word not in stop_words:\n",
    "                filtered_list.append(word)\n",
    "        sample_tweets.append(filtered_list)\n",
    "    return sample_tweets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets_train = lematizar(sample_tweets_train)\n",
    "sample_tweets_test = lematizar(sample_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['words']= sample_tweets_train\n",
    "test_df['words']= sample_tweets_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tratamento:\n",
    "    - de palavras erradas -> biblioteca usada no repositório abi-academy-hack\n",
    "    - letras repetidas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=['location', 'keyword','id','text'])\n",
    "train_df = train_df.drop(columns=['location', 'keyword','id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['words'].apply(lambda x: ' '.join([str(elem) for elem in x]))\n",
    "test_df['text'] = test_df['words'].apply(lambda x: ' '.join([str(elem) for elem in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     people receive wildfire evacuation order calif...\n",
       "4     got sent photo ruby alaska smoke wildfire pour...\n",
       "5     rockyfire update california hwy closed directi...\n",
       "6     flood disaster heavy rain cause flash flooding...\n",
       "7                                top hill see fire wood\n",
       "8     ha emergency evacuation happening building acr...\n",
       "9                            afraid tornado coming area\n",
       "10                      three people died heat wave far\n",
       "11    haha south tampa getting flooded hah- wait sec...\n",
       "12    raining flooding florida tampabay tampa day lo...\n",
       "13                      flood bago myanmar arrived bago\n",
       "14           damage school bus multi car crash breaking\n",
       "15                                               ha man\n",
       "16                                           love fruit\n",
       "17                                        summer lovely\n",
       "18                                             car fast\n",
       "19                                      goooooooaaaaaal\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'][3:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    " Definição: Bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. Machine learning algorithms cannot work with raw text directly; the text must be converted into well defined fixed-length(vector) numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formação da Bag-of-word para treino e teste com count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = train_df.head(6090)\n",
    "teste = train_df.tail(1523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_treino= treino['target']\n",
    "traget_teste= teste['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer()\n",
    "count_treino_cv = Vectorizer.fit_transform(treino['text'])\n",
    "count_teste_cv = Vectorizer.transform(teste['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_treino_cv[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formação da Bag-of-word para treino e teste com tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "count_treino_tfidf = tfidf_vectorizer.fit_transform(treino['text'])\n",
    "count_teste_tfidf = tfidf_vectorizer.transform(teste['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas(y_test,y_predicted):\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,average='weighted')             \n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cv = LogisticRegression(C=1, class_weight='balanced', solver='sag',multi_class='auto', n_jobs=-1, random_state=40,max_iter= 100000)\n",
    "clf_cv.fit(count_treino_cv, target_treino)\n",
    "Predição_target_cv = clf_cv.predict(count_teste_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7879185817465528, 0.7896749023950892, 0.7879185817465528, 0.7865201956024487)\n"
     ]
    }
   ],
   "source": [
    "print(metricas(traget_teste, Predição_target_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = LogisticRegression(C=1, class_weight='dict', solver='newton-cg',multi_class='auto', n_jobs=-1, random_state=40,max_iter= 1000)\n",
    "clf_tfidf.fit(count_treino_tfidf, target_treino)\n",
    "Predição_target_tfidf = clf_tfidf.predict(count_teste_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.788575180564675, 0.8037166405742918, 0.788575180564675, 0.7834546624011325)\n"
     ]
    }
   ],
   "source": [
    "print(metricas(traget_teste, Predição_target_tfidf))\n",
    "#Sem parâmetros ideias:\n",
    "#(0.7912015758371634, 0.7911097527493921, 0.7912015758371634, 0.7908037193499462)\n",
    "#(0.7925147734734077, 0.8075907785931117, 0.7925147734734077, 0.7875804286920929)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "'C': [1,2,3,300,500],\n",
    "'max_iter': [1000,100000],\n",
    "'gamma':['scale', 'auto'],\n",
    "'class_weight':['dict' ,'balanced']\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(\n",
    "        SVC(), parameters, scoring='accuracy'\n",
    "    )\n",
    "clf.fit(count_treino_tfidf, target_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 100000}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf',C=1,class_weight='balanced',gamma='scale', max_iter=100000) \n",
    "clf.fit(count_treino_cv, target_treino)\n",
    "Predição_target_SVM_cv = clf.predict(count_teste_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7820091923834537, 0.7990065430270551, 0.7820091923834537, 0.7761385531145388)\n"
     ]
    }
   ],
   "source": [
    "print(metricas(traget_teste, Predição_target_SVM_cv))\n",
    "#antes do grid search: (0.7544320420223244, 0.7607494234790165, 0.7544320420223244, 0.7505317206525575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf',C=1,class_weight='balanced',gamma='scale', max_iter=100000) \n",
    "clf.fit(count_treino_tfidf, target_treino)\n",
    "Predição_target_SVM_tfidf= clf.predict(count_teste_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7820091923834537, 0.7866863652510794, 0.7820091923834537, 0.7794820049168832)\n"
     ]
    }
   ],
   "source": [
    "print(metricas(traget_teste, Predição_target_SVM_tfidf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97df6f2fb7f9c55379e2d2444043f8d5f157c223a696fe5032f4316ff3fcd6e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
